{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Object Detection Examples\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img align=\"left\" src=\"https://media-exp1.licdn.com/dms/image/C4E1BAQFay3CGU2VmRg/company-background_10000/0/1621019049425?e=2159024400&v=beta&t=dmPCWeJlWvkzmM019V4_oKMluIPkQPX52i0zdgP-x2M\" alt=\"nn\" style=\"width: 800px;\"/>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img align=\"left\" src=\"https://media-exp1.licdn.com/dms/image/C4E0BAQHJY2WpOb492w/company-logo_200_200/0/1620816916063?e=2159024400&v=beta&t=U-VvNpjzV2DLp4EBIeqI8ZIWYUekPeJOQyxfXaJBMnU\" alt=\"nn\" style=\"width: 25px;\"/>\n",
    "\n",
    "Please visit us at https://onspecta.com\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## COCO Dataset Overview\n",
    "<img align=\"left\" src=\"https://cocodataset.org/images/coco-logo.png\" alt=\"nn\" style=\"width: 200px;\"/>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "These examples are using subset of COCO object detection validation set from year 2014.\n",
    "COCO is a large-scale object detection, segmentation, and captioning dataset.\n",
    "\n",
    "More info can be found here: https://cocodataset.org\n",
    "\n",
    "&nbsp;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.coco import COCODataset\n",
    "import utils.post_processing as pp\n",
    "import utils.benchmark as bench_utils\n",
    "\n",
    "BATCH_SIZE = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YOLO v4 Tiny in fp32 precision using TF2 api\n",
    "\n",
    "DLS offers a significant speed-up in standard fp32 inference scenarios.\n",
    "This example shows the performance of Yolo v4 Tiny model in fp32 precision.\n",
    "Original Yolo v4 paper can be found here: https://arxiv.org/pdf/2004.10934.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (416, 416)\n",
    "threshold = 0.3\n",
    "path_to_model = \"yolo_v4_tiny/yolo_v4_tiny_tf_fp32\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first let's load the model\n",
    "\n",
    "saved_model_loaded = tf.saved_model.load(path_to_model, tags=[tag_constants.SERVING])\n",
    "yolo = self.__saved_model_loaded.signatures['serving_default']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! DLS_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# setting the configuration\n",
    "tf.config.threading.set_intra_op_parallelism_threads(bench_utils.get_intra_op_parallelism_threads())\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_model=\"RGB\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\",\n",
    "    pre_processing=\"YOLO\"\n",
    ")\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "# warm-up run\n",
    "_ = yolo(tf.constant(coco.get_input_array(input_shape)))\n",
    "\n",
    "# actual run\n",
    "start = time.time()\n",
    "output = yolo(tf.constant(coco.get_input_array(input_shape)))\n",
    "finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nYOLO v4 Tiny FP32 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD Inception v2 in fp16 precision using TF1 api\n",
    "\n",
    "This example shows the performance of SSD Inception v2 model converted to fp16 precision.\n",
    "Models in fp16 precision are expected to offer accuracy on par with fp32 counterparts and up to 2x inference speed-up\n",
    "on compatible hardware when run with DLS. You can read more on SSD architecture here: https://arxiv.org/pdf/1512.02325.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (300, 300)\n",
    "threshold = 0.3\n",
    "path_to_model = \"ssd_inception_v2/ssd_inception_v2_tf_fp16.pb\"\n",
    "output_names = [\"detection_classes:0\", \"detection_boxes:0\", \"detection_scores:0\", \"num_detections:0\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first let's load the model\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! DLS_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# creating TF config\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.intra_op_parallelism_threads = bench_utils.get_intra_op_parallelism_threads()\n",
    "config.inter_op_parallelism_threads = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preparing input and output dictionaries\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {output_name: graph.get_tensor_by_name(output_name) for output_name in output_names}\n",
    "\n",
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_model=\"BGR\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\"\n",
    ")\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"image_tensor:0\"): coco.get_input_array(target_shape=input_shape)}\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_dls = sess.run(output_dict, feed_dict)\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD Inception v2 FP16 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS disabled\n",
    "\n",
    "tf.DLS.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_no_dls = sess.run(output_dict, feed_dict)\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD Inception v2 FP16 latency without DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# post-processing\n",
    "def post_process(image, output):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for d in range(int(output[\"num_detections:0\"][i])):\n",
    "\n",
    "            # the detected object does not exceed a set threshold we skip it\n",
    "            if output[\"detection_scores:0\"][i][d] < threshold:\n",
    "                continue\n",
    "\n",
    "            # first let's switch order of bbox boundaries from [top left bottom right] to [left top right bottom]\n",
    "            converted_bbox = coco.convert_bbox_to_coco_order(\n",
    "                output[\"detection_boxes:0\"][i][d] * input_shape[0],\n",
    "                1, 0, 3, 2,\n",
    "                absolute=False\n",
    "            )\n",
    "\n",
    "            # then rescale back to original image ratio\n",
    "            converted_bbox = coco.rescale_bbox(i, converted_bbox)\n",
    "\n",
    "            # we can now draw bbox on the original input image\n",
    "            image = pp.draw_bbox(image, converted_bbox, int(output[\"detection_classes:0\"][i][d]))\n",
    "\n",
    "    return image\n",
    "\n",
    "# show the post-processed images\n",
    "plt.imshow(cv2.cvtColor(post_process(img, output_dls), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"Output with DLS enabled\\n\")\n",
    "\n",
    "plt.imshow(cv2.cvtColor(post_process(img, output_no_dls), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"Output with DLS disabled\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}