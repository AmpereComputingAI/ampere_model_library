{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Object Detection Examples\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img align=\"center\" src=\"https://media-exp1.licdn.com/dms/image/C4E1BAQFay3CGU2VmRg/company-background_10000/0/1621019049425?e=2159024400&v=beta&t=dmPCWeJlWvkzmM019V4_oKMluIPkQPX52i0zdgP-x2M\" alt=\"nn\" style=\"width: 1200px;\"/>\n",
    "\n",
    "<img align=\"left\" src=\"https://media-exp1.licdn.com/dms/image/C4E0BAQHJY2WpOb492w/company-logo_200_200/0/1620816916063?e=2159024400&v=beta&t=U-VvNpjzV2DLp4EBIeqI8ZIWYUekPeJOQyxfXaJBMnU\" alt=\"nn\" style=\"width: 25px;\"/>\n",
    "\n",
    "&nbsp;&nbsp;Please visit us at https://onspecta.com\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## COCO Dataset Overview\n",
    "<img align=\"left\" src=\"https://cocodataset.org/images/coco-logo.png\" alt=\"nn\" style=\"width: 200px;\"/>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "These examples are using subset of COCO object detection validation set from year 2014.\n",
    "COCO is a large-scale object detection, segmentation, and captioning dataset.\n",
    "\n",
    "More info can be found here: https://cocodataset.org\n",
    "\n",
    "&nbsp;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "from utils.coco import COCODataset\n",
    "import utils.post_processing as pp\n",
    "import utils.benchmark as bench_utils\n",
    "\n",
    "LAT_BATCH_SIZE = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YOLO v4 Tiny in fp32 precision using TF2 api\n",
    "\n",
    "DLS offers a significant speed-up in standard fp32 inference scenarios.\n",
    "This example shows the performance of Yolo v4 Tiny model in fp32 precision.\n",
    "Original Yolo v4 paper can be found here: https://arxiv.org/pdf/2004.10934.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (416, 416)\n",
    "path_to_model = \"yolo_v4_tiny/yolo_v4_tiny_tf_fp32\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! DLS_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# setting the configuration - please not that this has to happen before initializing the model\n",
    "tf.config.threading.set_intra_op_parallelism_threads(bench_utils.get_intra_op_parallelism_threads())\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"RGB\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\",\n",
    "    pre_processing=\"YOLO\"\n",
    ")\n",
    "\n",
    "_ = coco.get_input_array(input_shape)\n",
    "input_tensor = tf.constant(coco.get_input_array(input_shape))\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's load the model\n",
    "\n",
    "saved_model_loaded = tf.saved_model.load(path_to_model, tags=[tag_constants.SERVING])\n",
    "yolo = saved_model_loaded.signatures['serving_default']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "# warm-up run\n",
    "_ = yolo(input_tensor)\n",
    "\n",
    "# actual run\n",
    "start = time.time()\n",
    "output_dls = yolo(input_tensor)\n",
    "finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nYOLO v4 Tiny FP32 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we have to reload the model\n",
    "\n",
    "saved_model_loaded = tf.saved_model.load(path_to_model, tags=[tag_constants.SERVING])\n",
    "yolo = saved_model_loaded.signatures['serving_default']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS disabled\n",
    "\n",
    "tf.DLS.force_disable()\n",
    "\n",
    "# warm-up run\n",
    "_ = yolo(input_tensor)\n",
    "\n",
    "# actual run\n",
    "start = time.time()\n",
    "output_no_dls = yolo(input_tensor)\n",
    "finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nYOLO v4 Tiny FP32 latency without DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "bboxes_dls = output_dls[\"tf.concat_12\"][:, :, 0:4]\n",
    "preds_dls = output_dls[\"tf.concat_12\"][:, :, 4:]\n",
    "\n",
    "bboxes_no_dls = output_no_dls[\"tf.concat_12\"][:, :, 0:4]\n",
    "preds_no_dls = output_no_dls[\"tf.concat_12\"][:, :, 4:]\n",
    "\n",
    "# post-processing\n",
    "def post_process(image, bboxes, preds):\n",
    "    detection_boxes, _, detection_classes, valid_detections = tf.image.combined_non_max_suppression(\n",
    "            boxes=tf.reshape(bboxes, (tf.shape(bboxes)[0], -1, 1, 4)),\n",
    "            scores=tf.reshape(\n",
    "                preds, (tf.shape(preds)[0], -1, tf.shape(preds)[-1])),\n",
    "            max_output_size_per_class=50,\n",
    "            max_total_size=50,\n",
    "            iou_threshold=0.45,\n",
    "            score_threshold=0.25\n",
    "        )\n",
    "\n",
    "    for i in range(LAT_BATCH_SIZE):\n",
    "        for d in range(int(valid_detections[i])):\n",
    "            # first let's switch order of bbox boundaries from [top left bottom right] to [left top right bottom]\n",
    "            converted_bbox = coco.convert_bbox_to_coco_order(\n",
    "                detection_boxes[i][d] * input_shape[0],\n",
    "                1, 0, 3, 2,\n",
    "                absolute=False\n",
    "            )\n",
    "\n",
    "            # then rescale back to original image ratio\n",
    "            converted_bbox = coco.rescale_bbox(i, converted_bbox)\n",
    "\n",
    "            # we can now draw bbox on the original input image\n",
    "            image = pp.draw_bbox(image, converted_bbox, int(detection_classes[i][d]))\n",
    "\n",
    "    return image\n",
    "\n",
    "# show the post-processed images\n",
    "plt.imshow(cv2.cvtColor(post_process(img, bboxes_dls, preds_dls), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"YOLO v4 Tiny FP32 output with DLS enabled\\n\")\n",
    "\n",
    "plt.imshow(cv2.cvtColor(post_process(img, bboxes_no_dls, preds_no_dls), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"YOLO v4 Tiny FP32 output with DLS disabled\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD Inception v2 in fp16 precision using TF1 api\n",
    "\n",
    "This example shows the performance of SSD Inception v2 model converted to fp16 precision.\n",
    "Models in fp16 precision are expected to offer accuracy on par with fp32 counterparts and up to 2x inference speed-up\n",
    "on compatible hardware when run with DLS. You can read more on SSD architecture here: https://arxiv.org/pdf/1512.02325.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (300, 300)\n",
    "threshold = 0.3\n",
    "path_to_fp16_model = \"ssd_inception_v2/ssd_inception_v2_tf_fp16.pb\"\n",
    "path_to_fp32_model = \"ssd_inception_v2/ssd_inception_v2_tf_fp32.pb\"\n",
    "output_names = [\"detection_classes:0\", \"detection_boxes:0\", \"detection_scores:0\", \"num_detections:0\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first let's load the model in fp16 precision\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_fp16_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! DLS_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# creating TF config\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.intra_op_parallelism_threads = bench_utils.get_intra_op_parallelism_threads()\n",
    "config.inter_op_parallelism_threads = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preparing input and output dictionaries\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {output_name: graph.get_tensor_by_name(output_name) for output_name in output_names}\n",
    "\n",
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"BGR\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\"\n",
    ")\n",
    "\n",
    "input_array = coco.get_input_array(target_shape=input_shape)\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"image_tensor:0\"): input_array}\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled in fp16 precision\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_dls = sess.run(output_dict, feed_dict)\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD Inception v2 FP16 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS disabled in fp16 precision\n",
    "\n",
    "tf.DLS.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_no_dls_fp16 = sess.run(output_dict, feed_dict)\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD Inception v2 FP16 latency without DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now let's load the model in fp32 precision for validation\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_fp32_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {output_name: graph.get_tensor_by_name(output_name) for output_name in output_names}\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"image_tensor:0\"): input_array}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS disabled in fp32 precision\n",
    "\n",
    "tf.DLS.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_no_dls_fp32 = sess.run(output_dict, feed_dict)\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD Inception v2 FP32 latency without DLS: {:.0f} ms\\n\".format(latency_ms))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# post-processing\n",
    "def post_process(image, output):\n",
    "    for i in range(LAT_BATCH_SIZE):\n",
    "        for d in range(int(output[\"num_detections:0\"][i])):\n",
    "\n",
    "            # the detected object does not exceed a set threshold we skip it\n",
    "            if output[\"detection_scores:0\"][i][d] < threshold:\n",
    "                continue\n",
    "\n",
    "            # first let's switch order of bbox boundaries from [top left bottom right] to [left top right bottom]\n",
    "            converted_bbox = coco.convert_bbox_to_coco_order(\n",
    "                output[\"detection_boxes:0\"][i][d] * input_shape[0],\n",
    "                1, 0, 3, 2,\n",
    "                absolute=False\n",
    "            )\n",
    "\n",
    "            # then rescale back to original image ratio\n",
    "            converted_bbox = coco.rescale_bbox(i, converted_bbox)\n",
    "\n",
    "            # we can now draw bbox on the original input image\n",
    "            image = pp.draw_bbox(image, converted_bbox, int(output[\"detection_classes:0\"][i][d]))\n",
    "\n",
    "    return image\n",
    "\n",
    "# show the post-processed images\n",
    "plt.imshow(cv2.cvtColor(post_process(img, output_dls), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"SSD Inception v2 FP16 output with DLS enabled\\n\")\n",
    "\n",
    "plt.imshow(cv2.cvtColor(post_process(img, output_no_dls_fp16), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"SSD Inception v2 FP16 output with DLS disabled\\n\")\n",
    "\n",
    "plt.imshow(cv2.cvtColor(post_process(img, output_no_dls_fp32), cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"SSD Inception v2 FP32 output with DLS disabled\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD MobileNet v2 in int8 precision using TFLite\n",
    "\n",
    "This example shows the performance of SSD MobileNet v2 model quantized to int8 precision with the use of TFLite converter.\n",
    "Models in mixed fp32/int8 precision as the one described here are expected to offer significant speed-up while allowing only a little degradation to accuracy.\n",
    "You can read more on SSD MobileNet architecture here: https://arxiv.org/pdf/1801.04381.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (300, 300)\n",
    "threshold = 0.3\n",
    "path_to_model = \"ssd_mobilenet_v2/ssd_mobilenet_v2_tflite_int8.tflite\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loading the .tflite model and initializing Interpreter\n",
    "\n",
    "interpreter = tf.lite.Interpreter(\n",
    "    model_path=path_to_model,\n",
    "    num_threads=bench_utils.get_intra_op_parallelism_threads()\n",
    ")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"BGR\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\",\n",
    "    pre_processing=\"SSD\"\n",
    ")\n",
    "\n",
    "for _ in range(2):\n",
    "    _ = coco.get_input_array(target_shape=input_shape)\n",
    "input_array = coco.get_input_array(target_shape=input_shape)\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "interpreter.set_tensor(input_details[0][\"index\"], input_array)\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "# warm-up run\n",
    "interpreter.invoke()\n",
    "\n",
    "# actual run\n",
    "start = time.time()\n",
    "interpreter.invoke()\n",
    "finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD MobileNet v2 INT8 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# post-processing\n",
    "def post_process(image, det_boxes, det_classes, det_scores, num_det):\n",
    "\n",
    "\n",
    "    for i in range(LAT_BATCH_SIZE):\n",
    "        for d in range(int(num_det)):\n",
    "\n",
    "            # the detected object does not exceed a set threshold we skip it\n",
    "            if det_scores[i][d] < threshold:\n",
    "                continue\n",
    "\n",
    "            # first let's switch order of bbox boundaries from [top left bottom right] to [left top right bottom]\n",
    "            converted_bbox = coco.convert_bbox_to_coco_order(\n",
    "                det_boxes[i][d] * input_shape[0],\n",
    "                1, 0, 3, 2,\n",
    "                absolute=False\n",
    "            )\n",
    "\n",
    "            # then rescale back to original image ratio\n",
    "            converted_bbox = coco.rescale_bbox(i, converted_bbox)\n",
    "\n",
    "            # we can now draw bbox on the original input image\n",
    "            image = pp.draw_bbox(image, converted_bbox, int(det_classes[i][d]))\n",
    "\n",
    "    return image\n",
    "\n",
    "detection_boxes = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "detection_classes = interpreter.get_tensor(output_details[1][\"index\"])\n",
    "detection_classes += 1  # model uses indexing from 0 while COCO dateset start with idx of 1\n",
    "detection_scores = interpreter.get_tensor(output_details[2][\"index\"])\n",
    "num_detections = interpreter.get_tensor(output_details[3][\"index\"])\n",
    "\n",
    "# show the post-processed images\n",
    "plt.imshow(cv2.cvtColor(\n",
    "    post_process(img, detection_boxes, detection_classes, detection_scores, num_detections),\n",
    "    cv2.COLOR_BGR2RGB\n",
    "))\n",
    "plt.show()\n",
    "print(\"SSD MobileNet v2 INT8 output with DLS enabled\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More examples can be run like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"numactl --cpunodebind=0 --membind=0 python3 /model_zoo/object_detection/ssd_mobilenet_v2/run.py -m /model_zoo/object_detection/ssd_mobilenet_v2/ssd_mobilenet_v2_tf_fp32.pb -p fp32 --timeout=5\")\n",
    "process = subprocess.Popen([\"numactl\", \"--cpunodebind=0\", \"--membind=0\", \"python3\", \"/model_zoo/object_detection/ssd_mobilenet_v2/run.py\", \"-m\", \"/model_zoo/object_detection/ssd_mobilenet_v2/ssd_mobilenet_v2_tf_fp32.pb\", \"-p\", \"fp32\", \"--timeout=5\"], stdout=subprocess.PIPE)\n",
    "stdout = process.communicate()[0]\n",
    "print(stdout.decode(\"utf-8\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}