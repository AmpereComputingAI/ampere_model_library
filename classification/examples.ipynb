{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Examples\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img align=\"center\" src=\"https://media-exp1.licdn.com/dms/image/C4E1BAQFay3CGU2VmRg/company-background_10000/0/1621019049425?e=2159024400&v=beta&t=dmPCWeJlWvkzmM019V4_oKMluIPkQPX52i0zdgP-x2M\" alt=\"nn\" style=\"width: 1200px;\"/>\n",
    "\n",
    "<img align=\"left\" src=\"https://media-exp1.licdn.com/dms/image/C4E0BAQHJY2WpOb492w/company-logo_200_200/0/1620816916063?e=2159024400&v=beta&t=U-VvNpjzV2DLp4EBIeqI8ZIWYUekPeJOQyxfXaJBMnU\" alt=\"nn\" style=\"width: 25px;\"/>\n",
    "\n",
    "&nbsp;&nbsp;Please visit us at https://onspecta.com\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## ImageNet Dataset Overview\n",
    "<img align=\"left\" src=\"https://www.image-net.org/static_files/index_files/logo.jpg\" alt=\"nn\" style=\"width: 200px;\"/>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "These examples are using subset of ImageNet classification validation set from year 2012.\n",
    "ImageNet is a large-scale classification dataset that has been instrumental in advancing computer vision and deep learning research.\n",
    "\n",
    "More info can be found here: https://image-net.org/\n",
    "\n",
    "&nbsp;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.imagenet import ImageNet\n",
    "import utils.post_processing as pp\n",
    "import utils.benchmark as bench_utils\n",
    "\n",
    "LAT_BATCH_SIZE = 1\n",
    "THROUGHPUT_BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Latency with ResNet-50 v1.5 in fp32 precision\n",
    "\n",
    "DLS offers a significant speed-up in standard fp32 inference scenarios.\n",
    "This example shows the performance of ResNet-50 v1.5 model in fp32 precision.\n",
    "Original ResNet paper can be found here: https://arxiv.org/pdf/1512.03385.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (224, 224)\n",
    "path_to_model = \"resnet_50_v15/resnet_50_v15_tf_fp32.pb\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first let's load the model\n",
    "\n",
    "graph = tf.compat.v1.Graph()\n",
    "with graph.as_default():\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(path_to_model, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        graph_def.ParseFromString(serialized_graph)\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! DLS_NUM_THREADS should be set prior to launching jupyter notebook !\n",
    "\n",
    "# creating TF config\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.intra_op_parallelism_threads = bench_utils.get_intra_op_parallelism_threads()\n",
    "config.inter_op_parallelism_threads = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# preparing input and output dictionaries\n",
    "\n",
    "# creation of output dictionary\n",
    "output_dict = {\"softmax_tensor:0\": graph.get_tensor_by_name(\"softmax_tensor:0\")}\n",
    "\n",
    "# initialization of ImageNet dataset\n",
    "imagenet = ImageNet(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"RGB\",\n",
    "    pre_processing=\"VGG\",\n",
    "    is1001classes=True\n",
    ")\n",
    "\n",
    "input_array = imagenet.get_input_array(target_shape=input_shape)\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "feed_dict = {graph.get_tensor_by_name(\"input_tensor:0\"): input_array}\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(imagenet.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled in fp32 precision\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_dls = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nResNet-50 v1.5 FP32 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS disabled in fp32 precision\n",
    "\n",
    "tf.DLS.force_disable()\n",
    "\n",
    "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
    "    # warm-up run\n",
    "    _ = sess.run(output_dict, feed_dict)\n",
    "\n",
    "    # actual run\n",
    "    start = time.time()\n",
    "    output_no_dls = sess.run(output_dict, feed_dict)[\"softmax_tensor:0\"]\n",
    "    finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nResNet-50 v1.5 FP32 latency without DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# show the image\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "print(\"ResNet-50 v1.5 FP32 predictions with DLS enabled:\\n\")\n",
    "\n",
    "print(f\"Top-1 prediction: {pp.get_imagenet_names(imagenet.extract_top1(output_dls[0]) + 1)}\")\n",
    "print(f\"Top-5 predictions: {pp.get_imagenet_names(imagenet.extract_top5(output_dls[0]) + 1)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD MobileNet v2 in int8 precision using TFLite\n",
    "\n",
    "This example shows the performance of SSD MobileNet v2 model quantized to int8 precision with the use of TFLite converter.\n",
    "Models in mixed fp32/int8 precision as the one described here are expected to offer significant speed-up while allowing only a little degradation to accuracy.\n",
    "You can read more on SSD MobileNet architecture here: https://arxiv.org/pdf/1801.04381.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape = (300, 300)\n",
    "threshold = 0.3\n",
    "path_to_model = \"ssd_mobilenet_v2/ssd_mobilenet_v2_tflite_int8.tflite\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loading the .tflite model and initializing Interpreter\n",
    "\n",
    "interpreter = tf.lite.Interpreter(\n",
    "    model_path=path_to_model,\n",
    "    num_threads=bench_utils.get_intra_op_parallelism_threads()\n",
    ")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialization of COCO dataset\n",
    "coco = COCODataset(\n",
    "    batch_size=LAT_BATCH_SIZE,\n",
    "    color_model=\"BGR\",\n",
    "    images_filename_base=\"COCO_val2014_000000000000\",\n",
    "    pre_processing=\"SSD\"\n",
    ")\n",
    "\n",
    "for _ in range(6):\n",
    "    _ = coco.get_input_array(target_shape=input_shape)\n",
    "input_array = coco.get_input_array(target_shape=input_shape)\n",
    "\n",
    "# assignment of input image to input tensor\n",
    "interpreter.set_tensor(input_details[0][\"index\"], input_array)\n",
    "\n",
    "# for the purpose of visualizing results let's load the image without pre-processing\n",
    "img = cv2.imread(str(coco.path_to_latest_image))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running the model with DLS enabled\n",
    "\n",
    "tf.DLS.force_enable()\n",
    "\n",
    "# warm-up run\n",
    "interpreter.invoke()\n",
    "\n",
    "# actual run\n",
    "start = time.time()\n",
    "interpreter.invoke()\n",
    "finish = time.time()\n",
    "\n",
    "latency_ms = (finish - start) * 1000\n",
    "print(\"\\nSSD MobileNet v2 INT8 latency with DLS: {:.0f} ms\\n\".format(latency_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualizing output\n",
    "\n",
    "# post-processing\n",
    "def post_process(image, det_boxes, det_classes, det_scores, num_det):\n",
    "\n",
    "\n",
    "    for i in range(LAT_BATCH_SIZE):\n",
    "        for d in range(int(num_det)):\n",
    "\n",
    "            # the detected object does not exceed a set threshold we skip it\n",
    "            if det_scores[i][d] < threshold:\n",
    "                continue\n",
    "\n",
    "            # first let's switch order of bbox boundaries from [top left bottom right] to [left top right bottom]\n",
    "            converted_bbox = coco.convert_bbox_to_coco_order(\n",
    "                det_boxes[i][d] * input_shape[0],\n",
    "                1, 0, 3, 2,\n",
    "                absolute=False\n",
    "            )\n",
    "\n",
    "            # then rescale back to original image ratio\n",
    "            converted_bbox = coco.rescale_bbox(i, converted_bbox)\n",
    "\n",
    "            # we can now draw bbox on the original input image\n",
    "            image = pp.draw_bbox(image, converted_bbox, int(det_classes[i][d]))\n",
    "\n",
    "    return image\n",
    "\n",
    "detection_boxes = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "detection_classes = interpreter.get_tensor(output_details[1][\"index\"])\n",
    "detection_classes += 1  # model uses indexing from 0 while COCO dateset start with idx of 1\n",
    "detection_scores = interpreter.get_tensor(output_details[2][\"index\"])\n",
    "num_detections = interpreter.get_tensor(output_details[3][\"index\"])\n",
    "\n",
    "# show the post-processed images\n",
    "plt.imshow(cv2.cvtColor(\n",
    "    post_process(img, detection_boxes, detection_classes, detection_scores, num_detections),\n",
    "    cv2.COLOR_BGR2RGB\n",
    "))\n",
    "plt.show()\n",
    "print(\"SSD MobileNet v2 INT8 output with DLS enabled\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD Inception v2 in fp16 precision using TF1 api\n",
    "\n",
    "This example shows the performance of SSD Inception v2 model converted to fp16 precision.\n",
    "Models in fp16 precision are expected to offer accuracy on par with fp32 counterparts and up to 2x inference speed-up\n",
    "on compatible hardware when run with DLS. You can read more on SSD architecture here: https://arxiv.org/pdf/1512.02325.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}